{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "from factfinder.calculate import Calculate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "##INPUTS\n",
    "pff_variable = 'mdhhinc'\n",
    "geotype = 'CT20'\n",
    "census_geoid_list = ['36005001901', '36005001902']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "pd.options.display.float_format = \"{:,.18f}\".format"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "try:\n",
    "    env_path = \"../.env\"\n",
    "    load_dotenv(dotenv_path=env_path)\n",
    "except:\n",
    "    print(\".env file is missing ...\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "calculate = Calculate(\n",
    "        api_key=os.environ[\"API_KEY\"], year=2019, source=\"acs\", geography='2010_to_2020'\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# See all digits of ratio\n",
    "ratio = calculate.geo.ratio\n",
    "print(ratio.dtypes)\n",
    "ratio.loc[ratio.geoid_ct2020.isin(census_geoid_list), :]\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "geoid_ct2010     object\n",
      "geoid_ct2020     object\n",
      "ratio           float64\n",
      "dtype: object\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  geoid_ct2010 geoid_ct2020                ratio\n",
       "4  36005001900  36005001901 0.245696400625978029\n",
       "5  36005001900  36005001902 0.754303599374022027"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geoid_ct2010</th>\n",
       "      <th>geoid_ct2020</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36005001900</td>\n",
       "      <td>36005001901</td>\n",
       "      <td>0.245696400625978029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>36005001900</td>\n",
       "      <td>36005001902</td>\n",
       "      <td>0.754303599374022027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# Get ranges and design factor from metadata\n",
    "ranges = calculate.meta.median_ranges(pff_variable)\n",
    "print(ranges)\n",
    "design_factor = calculate.meta.median_design_factor(pff_variable)\n",
    "print(f\"\\nDesign factor: {design_factor}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'mdhhiu10': [0, 9999], 'mdhhi10t14': [10000, 14999], 'mdhhi15t19': [15000, 19999], 'mdhhi20t24': [20000, 24999], 'mdhhi25t29': [25000, 29999], 'mdhhi30t34': [30000, 34999], 'mdhhi35t39': [35000, 39999], 'mdhhi40t44': [40000, 44999], 'mdhhi45t49': [45000, 49999], 'mdhhi50t59': [50000, 59999], 'mdhhi60t74': [60000, 74999], 'mdhhi75t99': [75000, 99999], 'mdhi100t124': [100000, 124999], 'mdhi125t149': [125000, 149999], 'mdhi150t199': [150000, 199999], 'mdhhi200pl': [200000, 9999999]}\n",
      "\n",
      "Design factor: 1.5\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# Calculate inputs in 2020 geogs\n",
    "df = calculate.calculate_e_m_multiprocessing(list(ranges.keys()), geotype)\n",
    "print(df.dtypes)\n",
    "print(df.loc[df.census_geoid.isin(census_geoid_list), :])"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ConnectionError",
     "evalue": "None: Max retries exceeded with url: /data/2019/acs/acs5?get=NAME%2CB19001_002E%2CB19001_002M&for=tract%3A%2A&key=6093c30de833695dc90f205f035f0d2eaae58f29&in=state%3A36+county%3A005 (Caused by None)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/vscode/.cache/pypoetry/virtualenvs/pff-factfinder-Gy5E4KFG-py3.9/lib/python3.9/site-packages/urllib3/connection.py\", line 169, in _new_conn\n    conn = connection.create_connection(\n  File \"/home/vscode/.cache/pypoetry/virtualenvs/pff-factfinder-Gy5E4KFG-py3.9/lib/python3.9/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n  File \"/usr/local/lib/python3.9/socket.py\", line 953, in getaddrinfo\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\nsocket.gaierror: [Errno -3] Temporary failure in name resolution\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/vscode/.cache/pypoetry/virtualenvs/pff-factfinder-Gy5E4KFG-py3.9/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 699, in urlopen\n    httplib_response = self._make_request(\n  File \"/home/vscode/.cache/pypoetry/virtualenvs/pff-factfinder-Gy5E4KFG-py3.9/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 382, in _make_request\n    self._validate_conn(conn)\n  File \"/home/vscode/.cache/pypoetry/virtualenvs/pff-factfinder-Gy5E4KFG-py3.9/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 1010, in _validate_conn\n    conn.connect()\n  File \"/home/vscode/.cache/pypoetry/virtualenvs/pff-factfinder-Gy5E4KFG-py3.9/lib/python3.9/site-packages/urllib3/connection.py\", line 353, in connect\n    conn = self._new_conn()\n  File \"/home/vscode/.cache/pypoetry/virtualenvs/pff-factfinder-Gy5E4KFG-py3.9/lib/python3.9/site-packages/urllib3/connection.py\", line 181, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fe87cb22850>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/vscode/.cache/pypoetry/virtualenvs/pff-factfinder-Gy5E4KFG-py3.9/lib/python3.9/site-packages/requests/adapters.py\", line 439, in send\n    resp = conn.urlopen(\n  File \"/home/vscode/.cache/pypoetry/virtualenvs/pff-factfinder-Gy5E4KFG-py3.9/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 755, in urlopen\n    retries = retries.increment(\n  File \"/home/vscode/.cache/pypoetry/virtualenvs/pff-factfinder-Gy5E4KFG-py3.9/lib/python3.9/site-packages/urllib3/util/retry.py\", line 574, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.census.gov', port=443): Max retries exceeded with url: /data/2019/acs/acs5?get=NAME%2CB19001_002E%2CB19001_002M&for=tract%3A%2A&key=6093c30de833695dc90f205f035f0d2eaae58f29&in=state%3A36+county%3A005 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fe87cb22850>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python3.9/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"/workspaces/db-factfinder/factfinder/calculate.py\", line 85, in calculate_e_m\n    df = self.d(from_geotype, pff_variable)\n  File \"/workspaces/db-factfinder/factfinder/download.py\", line 171, in __call__\n    df = self.download_variable(self.download_e_m, geotype, v)\n  File \"/workspaces/db-factfinder/factfinder/download.py\", line 57, in download_variable\n    dfs.append(download_function(geoquery, v))\n  File \"/workspaces/db-factfinder/factfinder/download.py\", line 99, in download_e_m\n    client.get(\n  File \"/home/vscode/.cache/pypoetry/virtualenvs/pff-factfinder-Gy5E4KFG-py3.9/lib/python3.9/site-packages/census/core.py\", line 298, in get\n    return super(ACSClient, self).get(*args, **kwargs)\n  File \"/home/vscode/.cache/pypoetry/virtualenvs/pff-factfinder-Gy5E4KFG-py3.9/lib/python3.9/site-packages/census/core.py\", line 155, in get\n    merged_results = [merge(result) for result in zip(*all_results)]\n  File \"/home/vscode/.cache/pypoetry/virtualenvs/pff-factfinder-Gy5E4KFG-py3.9/lib/python3.9/site-packages/census/core.py\", line 153, in <genexpr>\n    all_results = (self.query(fifty_fields, geo, year, **kwargs)\n  File \"/home/vscode/.cache/pypoetry/virtualenvs/pff-factfinder-Gy5E4KFG-py3.9/lib/python3.9/site-packages/census/core.py\", line 64, in wrapper\n    result = func(self, *args, **kwargs)\n  File \"/home/vscode/.cache/pypoetry/virtualenvs/pff-factfinder-Gy5E4KFG-py3.9/lib/python3.9/site-packages/census/core.py\", line 177, in query\n    resp = self.session.get(url, params=params)\n  File \"/home/vscode/.cache/pypoetry/virtualenvs/pff-factfinder-Gy5E4KFG-py3.9/lib/python3.9/site-packages/requests/sessions.py\", line 555, in get\n    return self.request('GET', url, **kwargs)\n  File \"/home/vscode/.cache/pypoetry/virtualenvs/pff-factfinder-Gy5E4KFG-py3.9/lib/python3.9/site-packages/requests/sessions.py\", line 542, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/home/vscode/.cache/pypoetry/virtualenvs/pff-factfinder-Gy5E4KFG-py3.9/lib/python3.9/site-packages/requests/sessions.py\", line 655, in send\n    r = adapter.send(request, **kwargs)\n  File \"/home/vscode/.cache/pypoetry/virtualenvs/pff-factfinder-Gy5E4KFG-py3.9/lib/python3.9/site-packages/requests/adapters.py\", line 516, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.census.gov', port=443): Max retries exceeded with url: /data/2019/acs/acs5?get=NAME%2CB19001_002E%2CB19001_002M&for=tract%3A%2A&key=6093c30de833695dc90f205f035f0d2eaae58f29&in=state%3A36+county%3A005 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fe87cb22850>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23856/3699833063.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Calculate inputs in 2020 geogs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_e_m_multiprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeotype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcensus_geoid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcensus_geoid_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspaces/db-factfinder/factfinder/calculate.py\u001b[0m in \u001b[0;36mcalculate_e_m_multiprocessing\u001b[0;34m(self, pff_variables, geotype)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0m_calculate_e_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_e_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeotype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeotype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdownload_pool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_calculate_e_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpff_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         '''\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: None: Max retries exceeded with url: /data/2019/acs/acs5?get=NAME%2CB19001_002E%2CB19001_002M&for=tract%3A%2A&key=6093c30de833695dc90f205f035f0d2eaae58f29&in=state%3A36+county%3A005 (Caused by None)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 3. create a pivot table with census_geoid as the index, and\n",
    "# pff_variable as column names. df_pivoted.e -> the estimation dataframe\n",
    "df_pivoted = df.loc[df.census_geoid.isin(census_geoid_list), [\"census_geoid\", \"pff_variable\", \"e\"]].pivot(\n",
    "    index=\"census_geoid\", columns=\"pff_variable\", values=[\"e\"]\n",
    ")\n",
    "print(df_pivoted.dtypes)\n",
    "df_pivoted = df_pivoted.round(16)\n",
    "print(df_pivoted)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Empty dataframe to store the results\n",
    "results = pd.DataFrame()\n",
    "results[\"census_geoid\"] = df_pivoted.index\n",
    "results[\"pff_variable\"] = pff_variable\n",
    "results[\"geotype\"] = geotype\n",
    "results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_median(ranges, row):\n",
    "    ordered = list(ranges.keys())\n",
    "    N = row[ordered].sum()\n",
    "    print(f\"\\n\\nN/2: {N/2}\")\n",
    "    C = 0\n",
    "    i = 0\n",
    "    while C <= N / 2 and i <= len(ranges.keys()) - 1:\n",
    "        print(f\"\\nRange i: {i}\")\n",
    "        C += row[ordered[i]]\n",
    "        print(f\"Cumulative frequency C: {C}\")\n",
    "        i += 1\n",
    "    i = i - 1\n",
    "    if i == 0:\n",
    "        print(\"N/2 is in first range\")\n",
    "        median = list(ranges.values())[0][1]\n",
    "        print(f\"Median: {median}\")\n",
    "    elif C == 0.0:\n",
    "        print(\"Cumulative frequency is 0\")\n",
    "        median = 0.0\n",
    "        print(f\"Median: {median}\")\n",
    "    elif i == len(ranges.keys()) - 1:\n",
    "        print(\"N/2 is in top range\")\n",
    "        median = list(ranges.values())[-1][0]\n",
    "        print(f\"Median: {median}\")\n",
    "    else:\n",
    "        print(f\"\\nN/2 is in range {i}\")\n",
    "        print(f\"Range {i}:\", ranges[ordered[i]])\n",
    "        C = C - row[ordered[i]]\n",
    "        print(f\"C_i-1: {C}\")\n",
    "        L = ranges[ordered[i]][0]\n",
    "        print(f\"L_i: {L}\")\n",
    "        F = row[ordered[i]]\n",
    "        print(f\"F_i: {F}\")\n",
    "        W = ranges[ordered[i]][1] - ranges[ordered[i]][0]\n",
    "        print(f\"W_i: {W}\")\n",
    "        median = L + (N / 2 - C) * W / F\n",
    "        print(f\"Median: {median}\")\n",
    "    return median"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_median_moe(ranges, row, DF=1.1):\n",
    "    md = row[\"e\"]\n",
    "    print(\"\\n\\n=======\")\n",
    "    print(f\"Median: {md}\\n\")\n",
    "    if md >= list(ranges.values())[-1][0]:\n",
    "        print(\"Median is above top bin lower value\")\n",
    "        return np.nan\n",
    "    else:\n",
    "        ordered = list(ranges.keys())\n",
    "        B = row[ordered].sum()\n",
    "        if B == 0:\n",
    "            print(\"Size of base is zero\")\n",
    "            return np.nan\n",
    "        else:\n",
    "            cumm_dist = list(np.cumsum(row[ordered]) / B * 100)\n",
    "            print(f\"Cumulative dist:\\n {cumm_dist}\")\n",
    "\n",
    "            se_50 = DF * (((93 / (7 * B)) * 2500)) ** 0.5\n",
    "            print(f\"SE of 50%: {se_50}\\n\\n\")\n",
    "\n",
    "            if se_50 >= 50:\n",
    "                return np.nan\n",
    "            else:\n",
    "                p_lower = 50 - se_50\n",
    "                print(f\"p_lower: {p_lower}\")\n",
    "                p_upper = 50 + se_50\n",
    "                print(f\"p_upper: {p_upper}\")\n",
    "\n",
    "                lower_bin = min([cumm_dist.index(i) for i in cumm_dist if i > p_lower])\n",
    "                print(f\"Bin containing p_lower: {lower_bin}\")\n",
    "                upper_bin = min([cumm_dist.index(i) for i in cumm_dist if i > p_upper])\n",
    "                print(f\"Bin containing p_upper: {upper_bin}\")\n",
    "\n",
    "                if lower_bin >= len(ordered) - 1:\n",
    "                    return np.nan\n",
    "                else:\n",
    "                    if lower_bin == upper_bin:\n",
    "                        print(\"\\nBoth bounds are in the same bin\\n\")\n",
    "                        A1 = min(ranges[ordered[lower_bin]])\n",
    "                        print(f\"Smallest value in the bin: {A1}\")\n",
    "                        A2 = min(ranges[ordered[lower_bin + 1]])\n",
    "                        print(f\"Largest value in the bin: {A2}\")\n",
    "                        C1 = cumm_dist[lower_bin - 1]\n",
    "                        print(f\"Cumulative percent of units less than smallest value: {C1}\")\n",
    "                        C2 = cumm_dist[lower_bin]\n",
    "                        print(f\"Cumulative percent of units less than largest value: {C2}\")\n",
    "                        lowerbound = (p_lower - C1) * (A2 - A1) / (C2 - C1) + A1\n",
    "                        upperbound = (p_upper - C1) * (A2 - A1) / (C2 - C1) + A1\n",
    "                        print(f\"Confidence interval: {lowerbound} to {upperbound}\")\n",
    "\n",
    "                    else:\n",
    "                        print(\"\\nBounds are in different bins\\n\")\n",
    "                        A1_l = min(ranges[ordered[lower_bin]])\n",
    "                        print(f\"Smallest value in the lower bin: {A1_l}\")\n",
    "                        A2_l = min(ranges[ordered[lower_bin + 1]])\n",
    "                        print(f\"Largest value in the lower bin: {A2_l}\")\n",
    "                        C1_l = cumm_dist[lower_bin - 1]\n",
    "                        print(f\"Cumulative percent of units less than lower bin smallest value: {C1_l}\")\n",
    "                        C2_l = cumm_dist[lower_bin]\n",
    "                        print(f\"Cumulative percent of units less than lower bin largest value: {C2_l}\")\n",
    "\n",
    "                        if upper_bin + 1 > len(ordered) - 1:\n",
    "                            print(\"\\nUpper bound is in top bin\")\n",
    "                            A1_u = min(ranges[ordered[upper_bin]])\n",
    "                            print(f\"Smallest value in the upper bin: {A1_u}\")\n",
    "                            A2_u = A1_u\n",
    "                            print(f\"Largest value in the upper bin: {A2_u}\")\n",
    "                            C1_u = cumm_dist[upper_bin - 1]\n",
    "                            print(f\"Cumulative percent of units less than upper bin smallest value: {C1_u}\")\n",
    "                            C2_u = cumm_dist[upper_bin]\n",
    "                            print(f\"Cumulative percent of units less than upper bin largest value: {C2_u}\")\n",
    "                        else:\n",
    "                            print(\"\\nUpper bound is below top bin\")\n",
    "                            A1_u = min(ranges[ordered[upper_bin]])\n",
    "                            print(f\"Smallest value in the upper bin: {A1_u}\")\n",
    "                            A2_u = min(ranges[ordered[upper_bin + 1]])\n",
    "                            print(f\"Largest value in the upper bin: {A2_u}\")\n",
    "                            C1_u = cumm_dist[upper_bin - 1]\n",
    "                            print(f\"Cumulative percent of units less than upper bin smallest value: {C1_u}\")\n",
    "                            C2_u = cumm_dist[upper_bin]\n",
    "                            print(f\"Cumulative percent of units less than upper bin largest value: {C2_u}\")\n",
    "\n",
    "                        lowerbound = (p_lower - C1_l) * (A2_l - A1_l) / (\n",
    "                            C2_l - C1_l\n",
    "                        ) + A1_l\n",
    "                        upperbound = (p_upper - C1_u) * (A2_u - A1_u) / (\n",
    "                            C2_u - C1_u\n",
    "                        ) + A1_u\n",
    "                        print(f\"Confidence interval: {lowerbound} to {upperbound}\")\n",
    "\n",
    "                    print(f\"MOE: {(upperbound - lowerbound) * 1.645 / 2}\")\n",
    "                    return (upperbound - lowerbound) * 1.645 / 2\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 4. calculate median estimation using get_median\n",
    "results[\"e\"] = (\n",
    "    df_pivoted.e.loc[\n",
    "        df_pivoted.e.index == results.census_geoid, list(ranges.keys())\n",
    "    ]\n",
    "    .apply(lambda row: get_median(ranges, row), axis=1)\n",
    "    .to_list()\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 5. Calculate median moe using get_median_moe\n",
    "# Note that median moe calculation needs the median estimation\n",
    "# so we seperated df_pivoted.m out as a seperate dataframe\n",
    "e = df_pivoted.e.copy()\n",
    "e[\"e\"] = results.loc[e.index == results.census_geoid, \"e\"].to_list()\n",
    "results[\"m\"] = (\n",
    "    e.loc[e.index == results.census_geoid, list(ranges.keys()) + [\"e\"]]\n",
    "    .apply(lambda row: get_median_moe(ranges, row, design_factor), axis=1)\n",
    "    .to_list()\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(results.dtypes)\n",
    "results.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Peform full calculation (including cleaning/rounding) to show display output\n",
    "\n",
    "full_calc = calculate(pff_variable, geotype)\n",
    "print(full_calc.dtypes)\n",
    "print(full_calc.loc[full_calc.census_geoid.isin(census_geoid_list),:])"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75371cd3c5ecd442c3beab879b92f620aa5f133f8c54bee4eee1a26c01085919"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('pff-factfinder-Gy5E4KFG-py3.9': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}